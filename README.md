# ğŸ§  DataFlowHub â€“ Lightweight ETL System (Django + Celery + Redis)

## ğŸ“˜ Overview

**DataFlowHub** is a lightweight ETL (Extractâ€“Transformâ€“Load) backend built with **Django REST Framework**, designed to automate data extraction and processing tasks.  
The project supports **asynchronous execution via Celery**, using **Redis** as a broker and backend for tasks.

The goal is to build a flexible foundation for ETL jobs that can be defined and triggered via API â€“ from various sources (APIs, databases, files).

---

## âš™ï¸ Features (so far)

âœ… REST API for defining and triggering ETL jobs  
âœ… Execution of ETL processes via **Celery tasks**  
âœ… **Redis** broker for asynchronous processing  
âœ… **Swagger/OpenAPI** documentation  
âœ… ETL jobs from **API sources**  
âœ… Automatic **transformation and normalization** of JSON with **pandas.json_normalize()**  
âœ… Execution logging and statuses (`pending`, `running`, `success`, `failed`)  
âœ… **Pagination** of results  
âœ… Modular architecture â€“ separation between `core/` and `etl/` apps

---

## ğŸ§© Planned Features

ğŸ”¹ ETL from **databases** (PostgreSQL, MySQL, etc.)  
ğŸ”¹ ETL from **file sources** (CSV, Excel, JSON)  
ğŸ”¹ **Authentication and Role-based access** â€“ different users with different permissions  
ğŸ”¹ **Filtering, searching, throttling** in API  
ğŸ”¹ **Docker** containerization  
ğŸ”¹ **.env configuration** for production  
ğŸ”¹ **CI/CD pipeline** (GitHub Actions)  
ğŸ”¹ **Flake8 / Ruff** for linting  
ğŸ”¹ **Pytest** for unit and integration tests  
ğŸ”¹ Full ETL configuration via **frontend interface** (optional)

---

## ğŸ§± Tech Stack

| Component       | Technology                       |
| --------------- | -------------------------------- |
| Backend         | Django 5 + Django REST Framework |
| Async Tasks     | Celery                           |
| Message Broker  | Redis                            |
| Data Processing | Pandas                           |
| Database        | SQLite (dev) â†’ PostgreSQL (prod) |
| API Docs        | drf-spectacular                  |
| Environment     | Python 3.12+                     |

---

## ğŸš€ Installation & Run (Local Dev)

```bash
# 1ï¸âƒ£ Clone repository
git clone https://github.com/yourusername/dataflowhub.git
cd dataflowhub

# 2ï¸âƒ£ Create and activate virtual environment
python -m venv .venv
source .venv/bin/activate  # on Windows: .venv\Scripts\activate

# 3ï¸âƒ£ Install dependencies
pip install -r requirements.txt

# 4ï¸âƒ£ Run Redis (Docker example)
docker run -d -p 6379:6379 redis

# 5ï¸âƒ£ Start Celery worker
celery -A dataflowhub worker -l info

# 6ï¸âƒ£ Run Django server
python manage.py runserver
```

Swagger Docs â†’ [http://127.0.0.1:8000/api/schema/swagger-ui/](http://127.0.0.1:8000/api/schema/swagger-ui/)

---

## ğŸ—ºï¸ Roadmap (Next Steps)

| Step                          | Description                               |
| ----------------------------- | ----------------------------------------- |
| ğŸ§© Add DB/File source support | Create specialized tasks and transformers |
| ğŸ”’ Add Authentication         | JWT or DRF Token-based system             |
| ğŸ§® Add Filtering & Searching  | Via DRF filters and query params          |
| ğŸ³ Add Docker setup           | docker-compose for web + redis + celery   |
| âš™ï¸ Add CI/CD                  | GitHub Actions workflow                   |
| ğŸ§ª Add Tests                  | With Pytest and Factory Boy               |
| ğŸ¯ Finalize Production Setup  | Environment configs, logs, and monitoring |

---

## ğŸ“‚ Project Structure

```
dataflowhub/
â”‚
â”œâ”€â”€ core/                  # Core configurations and shared utilities
â”œâ”€â”€ etl/                   # Main ETL logic (models, tasks, transformers)
â”‚   â”œâ”€â”€ models.py
â”‚   â”œâ”€â”€ tasks.py
â”‚   â”œâ”€â”€ transformer.py
â”‚   â””â”€â”€ views.py
â”‚
â”œâ”€â”€ dataflowhub/           # Django project config
â”‚
â””â”€â”€ README.md
```

---

## ğŸ§  Author & Vision

This project is built as a practical path toward a **production-ready Django REST backend**, with a real-world ETL use case.  
Once fully completed, it can be used as a foundation for:

- automated integrations between systems
- data synchronization
- backend for analytical dashboards

---

## ğŸ“œ License

MIT License Â© 2025
